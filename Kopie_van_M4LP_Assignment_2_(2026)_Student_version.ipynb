{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiekUU/compling/blob/main/Kopie_van_M4LP_Assignment_2_(2026)_Student_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zydk5m0FpaKT"
      },
      "source": [
        "## Assignment 2\n",
        "\n",
        "This is the complete Assignment 2. You are asked to train and test linear and logistic regression models and access lexical resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zydk5m0FpaKT"
      },
      "source": [
        "\n",
        "Group 23"
        "Wessel Kouwenhoven (exercise 2.1 & 2.2), Tijn Ezendam (exercise 2.3 & 2.4) & Niek Horstman (exercise 2.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ledaEclUpvzT"
      },
      "source": [
        "To start the assignment, import prerequisite packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRh6vltQpvTn"
      },
      "outputs": [],
      "source": [
        "import nltk,sklearn\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import collections, itertools\n",
        "import more_itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe word embeddings will be loaded through gensim package:"
      ],
      "metadata": {
        "id": "q1Gjwmay1urI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is all you need. **Do not import anything else at other points** except where suggested.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J45SbnKlSIbx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKO1jxm3pv93"
      },
      "source": [
        "# 2.1 Load GloVe word embedding model\n",
        "\n",
        "GloVe contains _static_ word embeddings. This means that the vector is assigned to word types and does not vary in different contexts or for different word senses. Pretrained GloVe word embedding models exist in different sizes. For the purpose of the exercise, we will use the smallest GloVe vectors with 50 dimensions. First, let's download the vectors and load the embedding model as `glove`. This may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import gensim.downloader as api"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RpbzSKOiuYge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "metadata": {
        "id": "z4GPth-uIwMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f79810f-5639-4ac5-ef1f-703b651896ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Another download workaround**. Sometimes StanfordNLP server is down and the embeddings don't load. In this case, you can obtain them from other sources. For example, download the embeddings from Kaggle (https://www.kaggle.com/datasets/rtatman/glove-global-vectors-for-word-representation?select=glove.6B.50d.txt), unzip the file and upoad it to the Colab working directory (left panel on Colab). Then uncomment and run:"
      ],
      "metadata": {
        "id": "QCFjiiY9FFiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from gensim.models import KeyedVectors\n",
        "#glove = KeyedVectors.load_word2vec_format('glove.6B.50d.txt', binary=False, no_header=True)"
      ],
      "metadata": {
        "id": "-nvSK6__CQ1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPoOZtlxqII4"
      },
      "source": [
        "The exercises below require understanding of gensim vector spaces. You can learn some basics from a tutorial on [word2vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html), which is an alternative to GloVe. For example, you will see in the tutorial that gensim word2vec has `wv.index_to_key` and `wv.key_to_index` attributes. By using a similar attribute of `glove`, we can count how many words the GloVe embeddings contain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JMWM-iTqHNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcec816e-b3b6-40f7-8ed0-6b88d71097a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(glove.key_to_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also check if particular orthographic words have vectors in `glove`.\n",
        "\n",
        "**Exercise**. Check if strings 'asdfdfasd', \"catc\", \"cact\", and \"cat\" have a glove vector, and print the resuts as truth values."
      ],
      "metadata": {
        "id": "6iaJetpi504A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "jfmuq9kNyRhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkVR4dXbqOSY"
      },
      "source": [
        "**Exercise**. What is the vector of _cat_? Print it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-EQf6-dqN_A"
      },
      "outputs": [],
      "source": [
        "#your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Which dimensions of vectors for \"cat\" and \"dog\" are similar, i.e. different by no more than 0.25? Print their indices. How many such dimensions are there? What is the proportion of similar dimensions out of all dimensions?\n",
        "\n"
      ],
      "metadata": {
        "id": "SznYrJv4KVOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code here\n",
        "\n",
        "print(\"Indices of similar dimensions:\",similar_dimensions)\n",
        "print(\"Number of similar dimensions:\",)\n",
        "print(\"Proportion of similar dimensions:\",)\n"
      ],
      "metadata": {
        "id": "KMkVBIXpOfDM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "6a377470-c2d9-4b13-e58b-ff701013d027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'similar_dimensions' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3817841307.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Indices of similar dimensions:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msimilar_dimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of similar dimensions:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Proportion of similar dimensions:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'similar_dimensions' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Compute the cosine of vectors for _cat_ and _dog_ using a gensim built-in function."
      ],
      "metadata": {
        "id": "Aqh_sjbD2i_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code here"
      ],
      "metadata": {
        "id": "-yoE2TmL2N4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byR4o0uZuYU0"
      },
      "source": [
        "# 2.2 Linear regression: Concreteness prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MudXBEHPqZQR"
      },
      "source": [
        "Obtain concreteness ratings from the paper\n",
        "\n",
        "Brysbaert, M., Warriner, A., & Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. BEHAVIOR RESEARCH METHODS, 46 (3), 904–911. https://doi.org/10.3758/s13428-013-0403-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cpty1jvvqZxR"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/ArtsEngine/concreteness/master/Concreteness_ratings_Brysbaert_et_al_BRM.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTWFbrn6q1gE"
      },
      "source": [
        "This is a tab-separated file with a header. Structured data like this can be conveniently read via DictReader class from csv."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koGwR3hfrNR_"
      },
      "source": [
        "Using DictReader, we create lists ```concreteness_words``` and ```concreteness_scores``` of words in the concreteness ratings file that have a GloVe vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwSaCcHBrhjf"
      },
      "outputs": [],
      "source": [
        "concreteness_words=[]\n",
        "concreteness_scores=[]\n",
        "with open(\"Concreteness_ratings_Brysbaert_et_al_BRM.txt\",'r') as concfile:\n",
        "  read_tsv = csv.DictReader(concfile, delimiter=\"\\t\")\n",
        "  for row in read_tsv:\n",
        "    word=row[\"Word\"]\n",
        "    if word in glove:\n",
        "      concreteness_words.append(word)\n",
        "      concreteness_scores.append(float(row[\"Conc.M\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many words ended up in your concreteness dataset?"
      ],
      "metadata": {
        "id": "0MQ7uWgvQxL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(concreteness_words))"
      ],
      "metadata": {
        "id": "_Gmj9W9VQAKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDKmvggQrq9X"
      },
      "source": [
        "We can now create train and test partitions of concreteness data (conc_words_train, conc_words_test, conc_scores_train, conc_scores_test) with 10% test and 90% training split. Do not use random state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCoOZ32trriI"
      },
      "outputs": [],
      "source": [
        "conc_words_train, conc_words_test, conc_scores_train, conc_scores_test = train_test_split(concreteness_words,concreteness_scores,test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKo12DqNr57Z"
      },
      "source": [
        "Convert data to torch tensor format to use in a regression model (ignore UserWarning):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IExC7ts2r6WF"
      },
      "outputs": [],
      "source": [
        "conc_vectors_train = torch.from_numpy(np.array([glove[word] for word in conc_words_train]))\n",
        "conc_vectors_test = torch.from_numpy(np.array([glove[word] for word in conc_words_test]))\n",
        "conc_scores_train = torch.tensor(conc_scores_train, dtype=torch.float32)\n",
        "conc_scores_test = torch.tensor(conc_scores_test, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise** Explain in a couple of sentences what the second and third lines of code in the above cell does.\n",
        "\n",
        "YOUR EXPLANATION HERE"
      ],
      "metadata": {
        "id": "RbIFJmZ7Vy8G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA41kZdKsKgR"
      },
      "source": [
        "Now we can define linear regression model in pyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1QduXYcsOj3"
      },
      "outputs": [],
      "source": [
        "class Regression(torch.nn.Module):\n",
        "     def __init__(self, input_dim, output_dim):\n",
        "         super(Regression, self).__init__()\n",
        "         self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "     def forward(self, x):\n",
        "         outputs = self.linear(x)\n",
        "         return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Study pyTorch documentation at\n",
        "\n",
        "https://docs.pytorch.org/tutorials/index.html\n",
        "\n",
        "to understand the definition of the regression class. Explain each line of the class definition above. For example, line 3 can be explained as follows:\n",
        "\n",
        "Line 3 `super(Regression, self).__init__()` initializes a new Regression model as an instance of the parent class `torch.nn.Module`\n",
        "\n",
        "Now explain all other lines of the class code.\n",
        "\n",
        "ADD YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "a2Hx3FTn7TWc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGGf2CJksYPp"
      },
      "source": [
        "A specific linear regression model can have the input dimensionality of our word embeddings and 1-dimensional input (the concretenss score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glovedim=50"
      ],
      "metadata": {
        "id": "-A5qxOl3K15t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfQIuGBqsXoG"
      },
      "outputs": [],
      "source": [
        "model = Regression(glovedim,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQcK2QCqsy4P"
      },
      "source": [
        "To train the model, we need a loss function and an optimiser, including the learning rate parameter. We choose Mean Square Error loss (suitable for learning linear regression) and Stochastic Gradient Descent method with the learning rate of 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJHqQcuaMcNL"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIuhn2FeNCph"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How PyTorch learns from data**\n",
        "\n",
        "Training a neural network means adjusting its parameters (weights and biases) so that its predictions get closer to the true values. PyTorch does this in four steps, repeated each epoch:\n",
        "\n",
        "- **Forward pass**: Input data flows through the model to produce predictions. Behind the scenes, PyTorch builds a computation graph — a record of every operation performed on the data.\n",
        "- **Loss calculation**: We measure how wrong the predictions are using a loss function (here, mean squared error).\n",
        "- **Backward pass** (`loss.backward()`): PyTorch walks backward through the computation graph, calculating gradients — these tell us how much each parameter contributed to the error, and in which direction it should change.\n",
        "- **Parameter update** (`optimizer.step()`): The optimizer adjusts each parameter by a small amount in the direction that reduces the loss.\n",
        "\n",
        "We call `zero_grad()` at the start because PyTorch accumulates gradients by default — without clearing them, gradients from previous epochs would mix with the current ones.\n",
        "\n",
        "A **computation graph** represents how outputs are computed from inputs, step by step. For training purposes, the inputs are model parameters (`linear.weight` and `linear.bias`) and the output is the loss value. A computation graph can be visualized in a diagram like:\n",
        "\n",
        "```\n",
        "  ┌───────────────────────┐\n",
        "  │   linear.weight       │\n",
        "  │   linear.bias         │\n",
        "  └───────────────────────┘\n",
        "              │\n",
        "              ▼\n",
        "           (Addmm)\n",
        "              │\n",
        "              ▼\n",
        "          (MseLoss)\n",
        "              │\n",
        "              ▼\n",
        "            loss\n",
        "            \n"
      ],
      "metadata": {
        "id": "O85a759PJxIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The computation graph (simplified for expository purposes) shows what PyTorch tracks: learnable parameters of the linear layer (weights and bias) udergo matrix multiplication with data and bias addition, then feed into the loss function.\n",
        "\n",
        "Why does the computation graph matter? To train the model, we need to know: \"If I nudge a weight slightly, how much does the loss change?\" When we call `loss.backward()`, PyTorch traverses this graph in reverse, from loss back to the parameters, computing gradients along the way. These gradients tell the optimizer how to adjust each parameter to reduce the loss."
      ],
      "metadata": {
        "id": "SVCOLO13NLO6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjjMEoVvtSc0"
      },
      "source": [
        "We can now train our linear regression model using gradient descent.\n",
        "\n",
        "**Exercise**. Every 5 training steps, evaluate the model, printing out loss and accuracy for the training and test sets. Calculate the accuracy as the percentage of examples where the predicted score of the model differs from the correct score by less than 1. The training may take a minute or so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDQGZSxWsyPH"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "losses_test = []\n",
        "Iterations = []\n",
        "epochs=20\n",
        "tolerance = 1\n",
        "#after defining parameters, we train the model several times on the same data; each iteration is an epoch:\n",
        "for epoch in trange(epochs, desc='Training Epochs'):\n",
        "    x = conc_vectors_train\n",
        "    scores = conc_scores_train\n",
        "    #clear gradients from previous epoch (PyTorch accumulates them by default)\n",
        "    optimizer.zero_grad()\n",
        "    #here we pass the word vectors from the training set, obtaining regression model's predicted outputs:\n",
        "    outputs = model(x)\n",
        "    #compute the loss: how far are predictions from true scores?\n",
        "    loss = criterion(torch.squeeze(outputs), scores)\n",
        "    #compute gradients: how should each parameter change to reduce the loss?\n",
        "    loss.backward()\n",
        "    #update model parameters using the computed gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    #Now, every 5 epochs we can evaluate how the model performs on the data\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        #we don't compute gradients as the model is only evaluated and not updated\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            #complete the code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The real computation graph used by pyTorch here is a little more complex than the schematic illustration we gave above; if you want to visualize it (using the familiar `graphviz` as the underlying engine), uncomment and run:"
      ],
      "metadata": {
        "id": "s2amIG-nNIyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torchviz\n",
        "#from torchviz import make_dot\n",
        "#make_dot(loss, params=dict(model.named_parameters()), show_attrs=False, show_saved=False)"
      ],
      "metadata": {
        "id": "ePPifiwLI5yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b>Click to see detailed explanation</b></summary>\n",
        "\n",
        "**Understanding the computation graph**\n",
        "\n",
        "The `torchviz` diagram shows the following nodes:\n",
        "\n",
        "| Node | What it is |\n",
        "|------|------------|\n",
        "| `linear.weight (1, 50)` | The learnable weight matrix of our linear layer: 50 input dimensions mapped to 1 output |\n",
        "| `linear.bias (1)` | The learnable bias term of our linear layer: a single value added to the output |\n",
        "| `AccumulateGrad` | A marker indicating \"this is a leaf node where gradients will be accumulated\" — appears next to each learnable parameter |\n",
        "| `TBackward0` | The backward operation for transpose (`T`): PyTorch transposes the weight matrix during the linear calculation |\n",
        "| `AddmmBackward0` | The backward operation for `addmm` (add + matrix multiply). This computes `bias + input @ weight.T`, which is what `nn.Linear` does internally |\n",
        "| `SqueezeBackward0` | The backward operation for `squeeze()`: shape adjustment from `(n, 1)` to `(n,)` |\n",
        "| `MseLossBackward0` | The backward operation for mean squared error loss |\n",
        "\n",
        "The \"Backward\" suffix indicates these are the *reverse* operations that PyTorch will use during backpropagation. The graph is built during the forward pass, but it stores the operations needed to compute gradients in reverse.\n",
        "\n",
        "Reading from bottom to top, the forward pass was:\n",
        "1. Take `linear.weight` and transpose it (`TBackward0`)\n",
        "2. Multiply input by transposed weights and add bias (`AddmmBackward0`)\n",
        "3. Squeeze the output shape (`SqueezeBackward0`)\n",
        "4. Compute MSE loss against target scores (`MseLossBackward0`)\n",
        "\n",
        "When we call `loss.backward()`, PyTorch traverses from top to bottom, computing gradients at each step until it reaches the `AccumulateGrad` nodes, where gradients are stored in `linear.weight.grad` and `linear.bias.grad`.\n",
        "</details>"
      ],
      "metadata": {
        "id": "y4x3sWCgOU1w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SJ3qAwKfDov"
      },
      "source": [
        "**Exercise**. What are the predicted concreteness score of the nouns _logarithm_ and _mouse_? Print them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z76FnkTpAeO3"
      },
      "outputs": [],
      "source": [
        "def predicted_concreteness(wd):\n",
        "  #complete the code here\n",
        "\n",
        "print(\"On the scale from 1 to 5, our trained regression model predicts the concreteness scoere of 'logarithm' to be\",predicted_concreteness(\"logarithm\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu2jvBh5fAmq"
      },
      "outputs": [],
      "source": [
        "print(\"On the scale from 1 to 5, our trained regression model predicts the concreteness scoere of 'mouse' to be\",predicted_concreteness(\"mouse\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6CTQkPAviBj"
      },
      "source": [
        "# 2.3. Create a dataset of WordNet supersenses for words that have GloVe vectors.\n",
        "\n",
        "First, download the WordNet database:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_TbA4Pfv9v5"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet as wn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can read documentation on object types in WordNet, for example by invoking the types:"
      ],
      "metadata": {
        "id": "gPVFTv6lngfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.corpus.reader.wordnet.Lemma"
      ],
      "metadata": {
        "id": "uLnPl63pnK-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.corpus.reader.wordnet.Synset"
      ],
      "metadata": {
        "id": "e5PBoC8invpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Now, retrieve the first lemma corresponding to the adjective _dry_ as `d`:"
      ],
      "metadata": {
        "id": "_nCIbS2Zn7WL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d="
      ],
      "metadata": {
        "id": "FzpyC3J9mYXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. What lemmas belong to the same SynSet? Retrieve the list."
      ],
      "metadata": {
        "id": "dhp5dHaeodHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code here"
      ],
      "metadata": {
        "id": "V29C-0zinUI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Define function `ant_freq` that returns the frequency of (the first) antonym of a lemma.\n"
      ],
      "metadata": {
        "id": "0_tAe2v8ojJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ant_freq(x):\n",
        "  #your code here"
      ],
      "metadata": {
        "id": "wrtQvLOgmzd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply `ant_freq` to `d`. This will output the frequency of _wet_."
      ],
      "metadata": {
        "id": "W63hTqccovBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ant_freq(d)"
      ],
      "metadata": {
        "id": "xbWZj8cqm3wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9UHFcCUwO_4"
      },
      "source": [
        "**Exercise**. Now, create a dataset that includes for each word in WordNet that has a GloVe vector the lexicographic file (supersense) of its first synset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C43EVE68ytWG"
      },
      "outputs": [],
      "source": [
        "#your code here\n",
        "wn_words =\n",
        "#wn_words = [w for w in wn.all_lemma_names() if w in glove]\n",
        "wn_supersenses ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHXhjXvjyt18"
      },
      "source": [
        "Split the dataset into train and test partitions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lrFM_48yW83"
      },
      "outputs": [],
      "source": [
        "wn_words_train, wn_words_test, wn_supersenses_train, wn_supersenses_test = train_test_split(wn_words,wn_supersenses,test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc0tWUqkugfe"
      },
      "source": [
        "# 2.4. Logistic regression: word class prediction.\n",
        "\n",
        "Now we can address a classification task. Define a (multinomial) regression model using softmax, choose a loss function and an optimizer for it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. We will need to set the number of classes for classification. Estimate `num_classes`prior to initializing the model. Use `wn_supersenses`."
      ],
      "metadata": {
        "id": "DtTnfSsCQNTd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX3nnLPlu8RZ"
      },
      "outputs": [],
      "source": [
        "num_classes ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAF9my4vfpYY"
      },
      "source": [
        "Initialize your model. Use the same Regression class for logistic regression as for linear regression - the difference will come from the objective (loss) function. The output now is not a single number but scores for each of the classes in the classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn7HvScVvazT"
      },
      "outputs": [],
      "source": [
        "logreg_model = Regression(glovedim,num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yxodwdYfzTl"
      },
      "source": [
        "**Exercise**. Choose the loss function. This is a crucial choice: some of the loss functions in PyTorch (see https://pytorch.org/docs/stable/nn.html) already include a softmax or a sigmoid in their implementaion, which gives computational advantages. Be sure to read the documentation on your loss function to confirm you made a good choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVRdqxAvvIPC"
      },
      "outputs": [],
      "source": [
        "#your code here\n",
        "loss4logreg ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H-JrZZVfzHF"
      },
      "source": [
        "**Exercise**.  Initialize the optimiser:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw6Pt_BKvTFx"
      },
      "outputs": [],
      "source": [
        "#your code here\n",
        "logregoptimiser ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSNhlheCk4xN"
      },
      "source": [
        "WordNet is quite big. For efficiency, use the following function for splitting your data into batches when processing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbA_NKV1k7vX"
      },
      "outputs": [],
      "source": [
        "batch_size = 200\n",
        "def get_batches(src_iter, tgt_iter, batch_size=batch_size):\n",
        "    for batch in more_itertools.chunked(zip(src_iter, tgt_iter), batch_size):\n",
        "        x, y = zip(*batch)\n",
        "        x = torch.stack(x)\n",
        "        y = torch.stack(y)\n",
        "        yield x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2bMjQiKy04C"
      },
      "source": [
        "**Exercise**. Train and test your logistic regression model, printing the train and test loss and accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpbabReyy3Gg"
      },
      "outputs": [],
      "source": [
        "#your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDWmbmFEh8em"
      },
      "source": [
        "**Exercise**. Define a mapping from indices to lexicographic file names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQFoIsxHgExM"
      },
      "outputs": [],
      "source": [
        "#your code\n",
        "itolexname ="
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the name of lexicographic file 2?"
      ],
      "metadata": {
        "id": "4VXxnXovdGUH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zriTsMNl-8PC"
      },
      "outputs": [],
      "source": [
        "itolexname[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eJ39Ocgik1Z"
      },
      "source": [
        "**Exercise**. Which supersense (lexicographic file) does your classifier assign to the noun _house_?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1JuvmYvgMyk"
      },
      "outputs": [],
      "source": [
        "def predicted_lexname(wd):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which supersense (lexicographic file) does your classifier assign to the noun _house_?"
      ],
      "metadata": {
        "id": "0YCvViZbRYHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_lexname(\"house\")"
      ],
      "metadata": {
        "id": "TB1mGbcoRZ5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRijRCGLiTyb"
      },
      "source": [
        "Which supersense (lexicographic file) does your classifier assign to the noun _hog_?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgePacYfgezw"
      },
      "outputs": [],
      "source": [
        "predicted_lexname(\"hog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL8uxCpyzah1"
      },
      "source": [
        "# 2.5. Hypernymy classification\n",
        "\n",
        "Now, download a lexical entailment (hypernymy) dataset called WBLESS. The dataset was developed by Weeds et al. with the goal of testing models on distinguishing hypernyms from other related word pairs.\n",
        "\n",
        "Weeds et al. (2014) Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir, and Bill Keller. 2014. Learning to distinguish hypernyms and co-hyponyms. In Proceedings of the 2014 International Conference on Computational Linguistics, pages 2249–2259, Dublin, Ireland.\n",
        "\n",
        "WBLESS (together with other relevant datasets) can be conveniently downloaded from the Facebook Research github page by Stephen Roller who worked on hypernymy learning:\n",
        "\n",
        "Stephen Roller, Douwe Kiela, and Maximilian Nickel. 2018. Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora. ACL.\n",
        "\n",
        "Download the tab-separated dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC37FAiq0TN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133f3c82-b05e-4565-d343-476f5e379272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-09 18:34:34--  https://github.com/facebookresearch/hypernymysuite/raw/main/data/wbless.tsv\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/facebookresearch/hypernymysuite/main/data/wbless.tsv [following]\n",
            "--2026-02-09 18:34:35--  https://raw.githubusercontent.com/facebookresearch/hypernymysuite/main/data/wbless.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 51267 (50K) [text/plain]\n",
            "Saving to: ‘wbless.tsv.2’\n",
            "\n",
            "wbless.tsv.2        100%[===================>]  50.07K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2026-02-09 18:34:35 (4.05 MB/s) - ‘wbless.tsv.2’ saved [51267/51267]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/facebookresearch/hypernymysuite/raw/main/data/wbless.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqCX7YDt27Vs"
      },
      "source": [
        "Check how the data looks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfLFN1110Zuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "474ae90f-180f-4e90-b038-94f539ede886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word1\tword2\tlabel\trelation\tfold\n",
            "frigate\tcraft\tTrue\thyper\tval\n",
            "trouble\tcarp\tFalse\tother\tval\n",
            "fox\tmouth\tFalse\tother\tval\n",
            "foot\trobin\tFalse\tother\tval\n",
            "vest\tgarment\tTrue\thyper\tval\n",
            "beetle\tinsect\tTrue\thyper\tval\n",
            "axe\tutensil\tTrue\thyper\tval\n",
            "chair\tscroll\tFalse\tother\tval\n",
            "end\tspear\tFalse\tother\tval\n"
          ]
        }
      ],
      "source": [
        "!head wbless.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8H2DTxmi45e"
      },
      "source": [
        "We used ```csv``` above to process a tab separated file. It can also be done using ```pandas```:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcKADH0wT-dJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "56cf8f0a-9331-4588-c095-e52d279e32ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     word1    word2  label relation fold\n",
              "0  frigate    craft   True    hyper  val\n",
              "1  trouble     carp  False    other  val\n",
              "2      fox    mouth  False    other  val\n",
              "3     foot    robin  False    other  val\n",
              "4     vest  garment   True    hyper  val"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7bb93807-c077-47a1-bd00-abdc27053b3e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word1</th>\n",
              "      <th>word2</th>\n",
              "      <th>label</th>\n",
              "      <th>relation</th>\n",
              "      <th>fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>frigate</td>\n",
              "      <td>craft</td>\n",
              "      <td>True</td>\n",
              "      <td>hyper</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>trouble</td>\n",
              "      <td>carp</td>\n",
              "      <td>False</td>\n",
              "      <td>other</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fox</td>\n",
              "      <td>mouth</td>\n",
              "      <td>False</td>\n",
              "      <td>other</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>foot</td>\n",
              "      <td>robin</td>\n",
              "      <td>False</td>\n",
              "      <td>other</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vest</td>\n",
              "      <td>garment</td>\n",
              "      <td>True</td>\n",
              "      <td>hyper</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7bb93807-c077-47a1-bd00-abdc27053b3e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7bb93807-c077-47a1-bd00-abdc27053b3e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7bb93807-c077-47a1-bd00-abdc27053b3e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "wbless_df",
              "summary": "{\n  \"name\": \"wbless_df\",\n  \"rows\": 1668,\n  \"fields\": [\n    {\n      \"column\": \"word1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 441,\n        \"samples\": [\n          \"cranberry\",\n          \"gill\",\n          \"vertebrate\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 498,\n        \"samples\": [\n          \"club\",\n          \"ambulance\",\n          \"runner-up\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relation\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"other\",\n          \"hyper\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fold\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"test\",\n          \"val\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "wbless_df = pd.read_csv('wbless.tsv', sep='\\t')\n",
        "wbless_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78WU2dWP3GvT"
      },
      "source": [
        "**Exercise**. Now create training and test data for relation classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kP1WKoj3HN2"
      },
      "outputs": [],
      "source": [
        "#your code here\n",
        "wbless_words = list(zip(wbless_df['word1'], wbless_df['word2']))\n",
        "hypernymy = wbless_df['label'].astype(int).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkUxGPQN2ODk"
      },
      "source": [
        "Split into training and test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5TADig-3VZ_"
      },
      "outputs": [],
      "source": [
        "wbless_words_train, wbless_words_test, hypernymy_train, hypernymy_test = train_test_split(wbless_words,hypernymy,test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZNxIFOc3kN9"
      },
      "source": [
        "**Exercise**. Create, train and test a logistic regression model that predicts whether two words stand in the hypernymy relation given their GloVe vectors.\n",
        "\n",
        "Make sure your model predicts a single score used for the binary decision (hypernymy vs. non-hypernymy) rather than scores for multiple classes, and choose the loss function in pyTorch accordingly.\n",
        "\n",
        "Print the train and test loss and accuracy. Use the concatenation of the two words' vectors as input to the logistic regression classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkYFJcWH320c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d0bf49-5fa5-404a-d305-5588b7268862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 01 | train loss 0.6474 acc 0.6316 | test loss 0.6463 acc 0.6707\n",
            "epoch 02 | train loss 0.5923 acc 0.7448 | test loss 0.5917 acc 0.7545\n",
            "epoch 03 | train loss 0.5472 acc 0.7928 | test loss 0.5498 acc 0.8024\n",
            "epoch 04 | train loss 0.5099 acc 0.8274 | test loss 0.5123 acc 0.8383\n",
            "epoch 05 | train loss 0.4794 acc 0.8461 | test loss 0.4831 acc 0.8623\n",
            "epoch 06 | train loss 0.4543 acc 0.8521 | test loss 0.4575 acc 0.8563\n",
            "epoch 07 | train loss 0.4329 acc 0.8621 | test loss 0.4359 acc 0.8683\n",
            "epoch 08 | train loss 0.4145 acc 0.8681 | test loss 0.4171 acc 0.8802\n",
            "epoch 09 | train loss 0.3988 acc 0.8721 | test loss 0.4004 acc 0.8743\n",
            "epoch 10 | train loss 0.3851 acc 0.8748 | test loss 0.3866 acc 0.8802\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "glove_model = glove\n",
        "embed_dim = glove_model.vector_size\n",
        "\n",
        "#returns zero vector if it's not in GloVe vocabulary\n",
        "def vector(w):\n",
        "    return glove_model[w].astype(np.float32) if w in glove_model else np.zeros(embed_dim, dtype=np.float32)\n",
        "\n",
        "def build(pairs, labels):\n",
        "    X = torch.tensor([np.concatenate([vector(w1), vector(w2)]) for w1, w2 in pairs], dtype=torch.float32) #every input is a concatenation of two word vectors\n",
        "    y = torch.tensor(labels, dtype=torch.float32).view(-1, 1)\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = build(wbless_words_train, hypernymy_train)\n",
        "X_test, y_test = build(wbless_words_test,  hypernymy_test)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(TensorDataset(X_test, y_test), batch_size=256, shuffle=False)\n",
        "\n",
        "model = nn.Linear(2*embed_dim, 1) #one linear layer that outputs single logit\n",
        "loss_fn = nn.BCEWithLogitsLoss() #bin classification with single logit\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    loss_sum, correct, n = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            logits = model(X)\n",
        "            loss_sum += loss_fn(logits, y).item() * len(y)\n",
        "            preds = (torch.sigmoid(logits) >= 0.5).float()\n",
        "            correct += (preds == y).sum().item()\n",
        "            n += len(y)\n",
        "    return loss_sum / n, correct / n\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for X, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(model(X), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_L, train_A = evaluate(train_loader)\n",
        "    test_L, test_A = evaluate(test_loader)\n",
        "    print(f\"epoch {epoch+1:02d} | train loss {train_L:.4f} acc {train_A:.4f} | test loss {test_L:.4f} acc {test_A:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYuoCyuWm-NF"
      },
      "source": [
        "**Exercise**. What label does your model predict for the pair _dog,animal_? Your code below should produce a Boolean value, `True` for the positive class (hypernymy) and `False` for the negative class (not hypernymy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Kujrk1QjIJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0dd978-f99c-43d6-d466-c79fa392b7a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted hypernymy, i.e. whether 'animal' is a hypernym of 'dog': True\n"
          ]
        }
      ],
      "source": [
        "def predicted_hypernymy(w1, w2):\n",
        "    x = np.concatenate([vector(w1), vector(w2)]).astype(np.float32)\n",
        "    x = torch.tensor(x).unsqueeze(0) #adding batch dimension shape which is needed for predicting single example\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logit = model(x)\n",
        "        probability = torch.sigmoid(logit).item()\n",
        "\n",
        "    return probability >= 0.5 #making the prediction binary\n",
        "\n",
        "\n",
        "print(\"Predicted hypernymy, i.e. whether 'animal' is a hypernym of 'dog':\", predicted_hypernymy(\"dog\", \"animal\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gR6R_6LnL05"
      },
      "source": [
        "What label does your model predict for the pair _dog,cat_?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n40gWmRjuf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a6cead-ff82-4148-a1a0-2c32246facbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted hypernymy, i.e. whether 'cat' is a hypernym of 'dog': False\n"
          ]
        }
      ],
      "source": [
        "print(\"Predicted hypernymy, i.e. whether 'cat' is a hypernym of 'dog':\",predicted_hypernymy(\"dog\",\"cat\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCKBTiCCnRRu"
      },
      "source": [
        "What label does your model predict for the pair _animal,dog_?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_9JK2EnnT2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a2871d-6071-4ef1-d7e4-9ceca2ac5426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted hypernymy, i.e. whether 'dog' is a hypernym of 'animal': False\n"
          ]
        }
      ],
      "source": [
        "print(\"Predicted hypernymy, i.e. whether 'dog' is a hypernym of 'animal':\",predicted_hypernymy(\"animal\",\"dog\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.6 Using FrameNet\n",
        "\n"
      ],
      "metadata": {
        "id": "-xR2nXnH5HJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can explore FrameNet online:\n",
        "https://framenet.icsi.berkeley.edu/frameIndex\n",
        "\n",
        "Or read detailed documentation here:\n",
        "https://framenet2.icsi.berkeley.edu/docs/r1.7/book.pdf\n",
        "\n",
        "Now, load FrameNet via the NLTK package:"
      ],
      "metadata": {
        "id": "nENYVvXQiwMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('framenet_v17')\n",
        "from nltk.corpus import framenet as fn"
      ],
      "metadata": {
        "id": "zyuhOo7m8n72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`fn.lus` allows to retrieve lexical units (LUs) recorded in FrameNet. A lexical unit approximately corresponds to a lemma in WordNet, i.e. a word taken in a specific sense. Without additional parameters, it returns a complete list:\n",
        "\n"
      ],
      "metadata": {
        "id": "K7PE47iCaR2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fn.lus()"
      ],
      "metadata": {
        "id": "ijewRC_JaShn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also retrieve lexical units by regular expression search. For example, the following returns the list of LUs that contain string _pres_ at the beginning of the LU name:"
      ],
      "metadata": {
        "id": "SC2XBdpyauNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fn.lus('^pres')"
      ],
      "metadata": {
        "id": "uJCGq6a181-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual lexical units can be retrieved by ID, for example:"
      ],
      "metadata": {
        "id": "aKfBkMyPbbLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fn.lu(10117)"
      ],
      "metadata": {
        "id": "LUpopayw9AFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terms that show up in square brackets are attributes, in this case of the lexical unit. They include usage examples:"
      ],
      "metadata": {
        "id": "9QHkZL7Xb2me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fn.lu(10117).exemplars"
      ],
      "metadata": {
        "id": "tzDT4Cuob5Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... and the frame that the lexical unit evokes, which in turn has its own attributes:"
      ],
      "metadata": {
        "id": "ef8vlLl_ckrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fn.lu(10117).frame"
      ],
      "metadata": {
        "id": "GWKLSgeEbqi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Find all LUs on FrameNet that share the frame with the noun *car*. Print these words along with their definitions."
      ],
      "metadata": {
        "id": "Cctg8so28msu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code here"
      ],
      "metadata": {
        "id": "-dOO4w5i5Lf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Define a function that takes a FrameNet frame as input and prints out the definitions of all core frame elements associated with the frame. For example, for the frame associated with the noun _car_ your function will print the definition of the only core frame element _Vehicle_:\n",
        "\n",
        "\n",
        "> Vehicle is the transportation device that the human beings use to travel.This FE is incorporated into each LU in this frame.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7EdFxMeWhdOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def printcoreFE(frame):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    frame: a frame object from FrameNet\n",
        "  \"\"\"\n",
        "  #your code here"
      ],
      "metadata": {
        "id": "cwp_SmRRflyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Test your function on the frame evoked by the verb _sing_:\n",
        "\n"
      ],
      "metadata": {
        "id": "fa_r8tEeiWtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code here\n",
        "sing=\n",
        "\n",
        "printcoreFE(frame=sing)"
      ],
      "metadata": {
        "id": "hpCNSPF6gkpt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
